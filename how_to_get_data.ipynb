{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596f4285-da30-4045-940f-5ddc82b6e7f1",
   "metadata": {},
   "source": [
    "\n",
    "### General Graph Structure\n",
    "\n",
    "The dataset will be structured as follows:\n",
    "- **`train_data`**, **`valid_data`**, and **`test_data`** share the same **edge_index**, which represents training edges in an **undirected** format (i.e., both `(u, v)` and `(v, u)` exist).\n",
    "- **`edge_type`**:\n",
    "  - `0` for **user-item** edges\n",
    "  - `1` for **item-user** edges\n",
    "- **`edge_attr`**: Represents edge features in `edge_index`.\n",
    "- **`target_edge_index`**: The directed edges in each split.\n",
    "  - For `train_data`, this is the directed version of `edge_index`.\n",
    "  - `target_edge_type` should be **all `0s`**.\n",
    "  - `target_edge_attr` contains features of `target_edge_index`.\n",
    "- **Additional attributes**:\n",
    "  - `num_nodes`, `num_users`, `num_items`: Should be set accordingly.\n",
    "  - `num_relations = 2` (user-item and item-user relationships).\n",
    "  - **Optional**: Include `x_user` and `x_item` as user and item features in `train_data`, `valid_data`, and `test_data`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6fc97-28c0-4110-a781-c9b15a2f2736",
   "metadata": {},
   "source": [
    "## Adding Yelp Gowalla\n",
    "Gowalla directly downloads all relevant files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104b9e5-1a12-4b89-bf5d-2fac3c28bfbf",
   "metadata": {},
   "source": [
    "## Adding Yelp Dataset\n",
    "#### **1.1 Interaction Data**\n",
    "The interaction data is directly downloaded in the code via raw_file link\n",
    "\n",
    "#### **1.2 Edge Features**\n",
    "Download the edge feature files from RecBole:\n",
    "[RecBole Yelp Dataset](https://recbole.io/dataset_list.html)\n",
    "\n",
    "Files to download:\n",
    "- `yelp_academic_dataset_business.json`\n",
    "- `yelp_academic_dataset_review.json`\n",
    "- `yelp_academic_dataset_user.json`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Move Files to the Correct Directory**\n",
    "\n",
    "After downloading, place all files into the following directory:\n",
    "```bash\n",
    "/itet-stor/trachsele/net_scratch/tl4rec/model_outputs/data/yelp18/raw\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a456b7-b02c-47ad-a54b-5feccd1f3633",
   "metadata": {},
   "source": [
    "## How to put ML-1M, BookX, Epinions, and LastFM Datasets in CSV files\n",
    "\n",
    "### **1. Download and Organize the Datasets**\n",
    "\n",
    "We will use the following repository for dataset preparation:\n",
    "[https://github.com/recsys-benchmark/DaisyRec-v2.0/tree/dev](https://github.com/recsys-benchmark/DaisyRec-v2.0/tree/dev)\n",
    "\n",
    "#### **Steps to Download**\n",
    "1. Clone the DaisyRec repository:\n",
    "   ```bash\n",
    "   git clone --branch dev https://github.com/recsys-benchmark/DaisyRec-v2.0.git\n",
    "   ```\n",
    "2. Navigate into the directory:\n",
    "   ```bash\n",
    "   cd DaisyRec-v2.0\n",
    "   ```\n",
    "3. Place your dataset files into the folder specified in the repository's documentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1f4a2-b1b3-4b95-952f-e80ccf512394",
   "metadata": {},
   "source": [
    "### **2. Modify `loader.py` to Include More Edge Features**\n",
    "\n",
    "Edit the file `daisy/utils/loader.py` to modify dataset loading for LastFM, BookX, and Epinions to obtain more edge features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a47a0-7af7-45d1-856e-4d714fef72e3",
   "metadata": {},
   "source": [
    "#### **Modify the LastFM Loader**\n",
    "```python\n",
    "elif self.src == 'lastfm':\n",
    "    df = pd.read_csv(f'{self.ds_path}user_artists.dat', sep='\\t')\n",
    "    df.rename(columns={'userID': self.uid_name, 'artistID': self.iid_name, 'weight': self.inter_name}, inplace=True)\n",
    "    # Fake timestamp column\n",
    "    df[self.tid_name] = 1\n",
    "```\n",
    "\n",
    "#### **Modify the BookX Loader**\n",
    "```python\n",
    "elif self.src == 'book-x':\n",
    "    df = pd.read_csv(f'{self.ds_path}BX-Book-Ratings.csv', delimiter=\";\", encoding=\"latin1\")\n",
    "    df.rename(columns={'User-ID': self.uid_name, 'ISBN': self.iid_name, 'Book-Rating': self.inter_name}, inplace=True)\n",
    "    # Fake timestamp column\n",
    "    df[self.tid_name] = 1\n",
    "```\n",
    "\n",
    "#### **Modify the Epinions Loader**\n",
    "```python\n",
    "elif self.src == 'epinions':\n",
    "    d = sio.loadmat(f'{self.ds_path}rating_with_timestamp.mat')\n",
    "    prime = []\n",
    "    for val in d['rating_with_timestamp']:\n",
    "        user, item, category, rating, helpfulness, timestamp = val[0], val[1], val[2], val[3], val[4], val[5]\n",
    "        prime.append([user, item, category, rating, helpfulness, timestamp])\n",
    "    \n",
    "    # Add category and helpfulness to DataFrame\n",
    "    df = pd.DataFrame(prime, columns=[self.uid_name, self.iid_name, \"category\", self.inter_name, \"helpfulness\", self.tid_name])\n",
    "    del prime\n",
    "    gc.collect()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403c992-97a5-4afb-9ada-3e49f117688b",
   "metadata": {},
   "source": [
    "### **3. Modify `test.py` to Save Processed CSV Files**\n",
    "\n",
    "Edit `test.py` to include dataset splitting and saving:\n",
    "\n",
    "Find the following line:\n",
    "```python\n",
    "splitter = TestSplitter(config)\n",
    "train_index, test_index = splitter.split(df)\n",
    "```\n",
    "\n",
    "Immediately after, add:\n",
    "```python\n",
    "df['split'] = 'none'\n",
    "df.loc[train_index, 'split'] = 'train'\n",
    "df.loc[test_index, 'split'] = 'test'\n",
    "# Save the CSV file\n",
    "# df.to_csv('epinions_raw_with_splits.csv', index=False)\n",
    "```\n",
    "\n",
    "This ensures the CSV files contain the necessary split information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940ecd6-5a51-42c1-b13b-deea3e70a16b",
   "metadata": {},
   "source": [
    "### **4. Update `basic.yaml` for Each Dataset**\n",
    "\n",
    "Modify the file `daisy/assets/basic.yaml` based on the dataset you are using:\n",
    "\n",
    "#### **General Changes**\n",
    "```yaml\n",
    "dataset: 'ml-1m'   # Change this to your dataset name\n",
    "prepro: 10filter  # Removes all items with fewer than 10 interactions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7506bf-7e6a-4c94-85e0-9093efbb3860",
   "metadata": {},
   "source": [
    "#### **Splitting Methods**\n",
    "- **For ML-1M and Epinions:**\n",
    "  ```yaml\n",
    "  val_method: 'tsbr'  # Time-aware split by ratio\n",
    "  test_method: 'tsbr'\n",
    "  ```\n",
    "- **For BookX and LastFM:**\n",
    "  ```yaml\n",
    "  val_method: 'rsbr'  # Random split by ratio\n",
    "  test_method: 'rsbr'\n",
    "  ```\n",
    "\n",
    "#### **Positive Threshold for Ratings**\n",
    "- **For ML-1M:**\n",
    "  ```yaml\n",
    "  positive_threshold: 4  # Ratings < 4 are not treated as interactions\n",
    "  ```\n",
    "- **For Other Datasets:**\n",
    "  ```yaml\n",
    "  positive_threshold: ~  # No filtering applied\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb6de3-6147-48c2-accc-afa7d26e787a",
   "metadata": {},
   "source": [
    "\n",
    "### **5. Run the Code and Save the CSV Files**\n",
    "\n",
    "Once the changes are made, run the following to process and split datasets:\n",
    "```bash\n",
    "python test.py\n",
    "```\n",
    "\n",
    "After running the script, locate the generated CSV file in the folder specified by the repository's config:\n",
    "```bash\n",
    "/itet-stor/trachsele/net_scratch/tl4rec/model_outputs/data/{dataset_name}/raw\n",
    "```\n",
    "\n",
    "Now the datasets are in the required CSV format and ready to use! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8952f-dc52-4b10-b7f1-bf21118e4330",
   "metadata": {},
   "source": [
    "## How to Prepare the Dataset from a CSV file\n",
    "Used for adding your own dataset or for adding ML-1M, Epinions, BookX, and LastFM\n",
    "\n",
    "### 1. Dataset Requirements\n",
    "\n",
    "Your dataset should be in a CSV file with the following columns:\n",
    "- **user**: Integer representing the user ID (0 to `num_users - 1`).\n",
    "- **item**: Integer representing the item ID (0 to `num_items - 1`).\n",
    "- **split**: Specifies whether the interaction belongs to the `\"train\"` or `\"test\"` set.\n",
    "- **feature1, feature2, ...**: Additional edge features (e.g., rating, timestamp, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aab2ff-0526-4b90-894d-85f8e7473285",
   "metadata": {},
   "source": [
    "### 2. Using `BaseRecDataset`\n",
    "\n",
    "If your dataset meets the above structure, you can use our `BaseRecDataset` class to process it. \n",
    "This class provides:\n",
    "- **Automatic loading and processing** of the CSV file.\n",
    "- **Splitting data into train/test/validation**.\n",
    "- **Handling user and item features**.\n",
    "- **Creating an undirected graph for training edges**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3ed50-9e5d-49fe-9be3-3cbb497ece2e",
   "metadata": {},
   "source": [
    "Subclasses should override:\n",
    "- `custom_preprocessing(self, df)`: To handle dataset-specific modifications.\n",
    "- `get_meta_info(self)`: To define metadata for edge features.\n",
    "- `train_split_ratio()` and `valid_split_ratio()`: If custom split ratios are needed.\n",
    "- `raw_file_names`: If the raw file has a different name.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec514a-97b2-46b1-ae06-687ce2532ba3",
   "metadata": {},
   "source": [
    "### 3. Example: Using `Ml1m` Dataset\n",
    "\n",
    "```python\n",
    "class Ml1m(BaseRecDataset):\n",
    "    dataset_name = \"ml-1m\"\n",
    "    \n",
    "    def custom_preprocessing(self, df):\n",
    "        # Rename \"items\" to \"item\" if needed.\n",
    "        df = super().custom_preprocessing(df)\n",
    "        return df\n",
    "        \n",
    "    def get_meta_info(self):\n",
    "        # Define metadata for the dataset\n",
    "        meta_info = preprocess_data.get_meta_info()\n",
    "        meta_info[\"numerical_cols\"] = [\"rating\", \"timestamp\"]\n",
    "        meta_info[\"drop_cols\"] = [\"user\", \"item\"]\n",
    "        return meta_info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b241a-722d-4407-b131-40990b18dd90",
   "metadata": {},
   "source": [
    "Here, `get_meta_info()` specifies:\n",
    "- **`numerical_cols`**: `rating` and `timestamp` are numerical features.\n",
    "- **`drop_cols`**: `user` and `item` should not be processed as edge features.\n",
    "\n",
    "---\n",
    "### 4. Metadata Structure (`get_meta_info()`)\n",
    "\n",
    "`meta_info` provides information about feature types:\n",
    "```python\n",
    "def get_meta_info():\n",
    "    meta_info = {}\n",
    "    meta_info[\"numerical_cols\"] = []  # Numerical features\n",
    "    meta_info[\"date_cols\"] = []  # Expected in \"%Y-%m-%d %H:%M:%S\"\n",
    "    meta_info[\"categorical_cols\"] = []  # Low-cardinality categorical features\n",
    "    meta_info[\"str_cols\"] = []  # High-cardinality strings (e.g., names)\n",
    "    meta_info[\"drop_cols\"] = []  # Features to drop (e.g., names)\n",
    "    meta_info[\"ls_of_cat_string\"] = []  # List of categorical strings per entry\n",
    "    meta_info[\"dont_touch\"] = []  # Features that should remain unchanged\n",
    "    return meta_info\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34cee7-764c-46dd-96bc-f985188279d8",
   "metadata": {},
   "source": [
    "## Adding Amazon Datasets\n",
    "\n",
    "### **1. Download the Amazon Datasets**\n",
    "\n",
    "The Amazon datasets can be downloaded from the following link:\n",
    "[Google Drive - Amazon Datasets](https://drive.google.com/drive/folders/1a_u52mIEUA-1WrwsNZZa-aoGJcMmVugs)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Organize the Files**\n",
    "\n",
    "Once downloaded, move the dataset files into the appropriate folder in our repository.\n",
    "\n",
    "For each dataset, rename the files as follows:\n",
    "\n",
    "#### **Fashion Dataset**\n",
    "| Original Filename | New Filename |\n",
    "|-------------------|-------------|\n",
    "| `Fashion.txt` | `amazon_fashion.txt` |\n",
    "| `CXTDictSasRec_Fashion.dat` | `amazon_fashion_ctxt.dat` |\n",
    "| `Fashion_imgs.dat` | `amazon_fashion_feat_cat.dat` |\n",
    "\n",
    "#### **Other Datasets**\n",
    "Similarly, apply the same renaming pattern for other datasets:\n",
    "\n",
    "### **3. Move to the Correct Folder**\n",
    "\n",
    "Ensure that the renamed files are placed in the dataset directory as specified in the repository configuration.\n",
    "\n",
    "\n",
    "\n",
    "Now the Amazon datasets are properly organized and ready for use! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a33cea-c13f-4b25-8c5a-70e27f6a0471",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3990926-d7ef-43ca-bcef-bf660d902ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba_bugfix",
   "language": "python",
   "name": "ba_bugfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
