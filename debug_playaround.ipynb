{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420fee2-0240-4834-a5d7-55c66184323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import MovieLens1M, MovieLens100K\n",
    "\n",
    "\n",
    "# Load MovieLens 1M dataset (adjust root if needed) push test\n",
    "#dataset = MovieLens1M(root='/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/temp_pyg')\n",
    "dataset= MovieLens100K(root=\"/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/temp_pyg\")\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a4db48-f865-42f6-abe7-7c5145a6198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Embedding Tensor:\n",
      " tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.4000],\n",
      "        [0.5000, 0.6000]])\n",
      "Item Embedding Tensor:\n",
      " tensor([[0.7000, 0.8000],\n",
      "        [0.9000, 1.0000],\n",
      "        [1.1000, 1.2000]])\n",
      "Head Embeddings:\n",
      " tensor([[[0.1000, 0.2000],\n",
      "         [0.1000, 0.2000],\n",
      "         [0.1000, 0.2000]],\n",
      "\n",
      "        [[0.3000, 0.4000],\n",
      "         [0.5000, 0.6000],\n",
      "         [0.1000, 0.2000]]])\n",
      "Tail Embeddings:\n",
      " tensor([[[0.7000, 0.8000],\n",
      "         [0.9000, 1.0000],\n",
      "         [1.1000, 1.2000]],\n",
      "\n",
      "        [[0.9000, 1.0000],\n",
      "         [0.9000, 1.0000],\n",
      "         [0.9000, 1.0000]]])\n",
      "size head Embeddings:\n",
      " torch.Size([2, 3, 2])\n",
      "size Tail Embeddings:\n",
      " torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "num_negatives = 2\n",
    "num_users = 3\n",
    "num_items = 3\n",
    "dim = 2\n",
    "\n",
    "# Embeddings for users and items (3 users, 3 items, embedding dimension 2)\n",
    "user_embedding = torch.tensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "item_embedding = torch.tensor([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])\n",
    "\n",
    "# h_index and t_index based on the edges and negative samples\n",
    "# Each row represents a batch entry, and each column is a negative sample\n",
    "# Here, `0` and `1` are valid head indices (users), and we corrupt them by keeping valid and invalid tails\n",
    "h_index = torch.tensor([\n",
    "    [0, 0, 0],  # For first edge (0, 0, 3) with two negatives\n",
    "    [1, 2, 0]   # For second edge (1, 0, 4) with two negatives\n",
    "])\n",
    "t_index = torch.tensor([\n",
    "    [3, 4, 5],  # Original (0, 0, 3), and corrupted tails [4, 2]\n",
    "    [4, 4, 4]   # Original (1, 0, 4), and corrupted tails [5, 0]\n",
    "])\n",
    "\n",
    "# Gather head node embeddings\n",
    "# (num_nodes, dim)\n",
    "index_temp = h_index.unsqueeze(-1).expand(-1, -1, user_embedding.shape[-1])\n",
    "h_embeddings = user_embedding.unsqueeze(0).expand(batch_size,-1,-1).gather(1, h_index.unsqueeze(-1).expand(-1, -1, dim))\n",
    "\n",
    "# Adjust `t_index` to map to item IDs by subtracting `num_users`\n",
    "index_temp = (t_index - num_users).clamp(min=0)\n",
    "t_embeddings = item_embedding.unsqueeze(0).expand(batch_size,-1,-1).gather(1, index_temp.unsqueeze(-1).expand(-1, -1, dim))\n",
    "\n",
    "print(\"User Embedding Tensor:\\n\", user_embedding)\n",
    "print(\"Item Embedding Tensor:\\n\", item_embedding)\n",
    "print(\"Head Embeddings:\\n\", h_embeddings)\n",
    "print(\"Tail Embeddings:\\n\", t_embeddings)\n",
    "print(\"size head Embeddings:\\n\", h_embeddings.shape)\n",
    "print(\"size Tail Embeddings:\\n\", t_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60bc3c5-a16b-4780-8342-c6f78b21f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "plot_ratings_vs_time(data)\n",
    "#print (data)\n",
    "#print (data['user', 'rates', 'movie'].rating[:20])\n",
    "#print (graph[\"movie\"].x[:10,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9134d9-f7d4-4abd-82de-c51570b0aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0.])]\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t_ranking = torch.zeros(5)\n",
    "h_ranking = torch.zeros(6)\n",
    "rankings1 = [t_ranking, h_ranking]\n",
    "print(rankings1)\n",
    "rankings2 = torch.cat([t_ranking, h_ranking], dim=0)\n",
    "print(rankings2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130c7989-98c0-4e7e-bbe7-04481f50f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_features = graph[\"user\"].x\n",
    "print (movies_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d51b938-df62-4402-bbd5-f3832f447c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2588a-3d3a-4878-8a13-a10a92d70c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run script/pretrain.py -c config/recommender/pretrain_notebook.yaml --gpus [0] /itet-stor/trachsele/net_scratch/tl4rec/ckpts/pretrain/BookX.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6aac0-77b8-4bbe-910c-f41567b4d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/trachsele/net_scratch/conda_envs/ba_bugfix/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "14:51:10   Random seed: 1024\n",
      "14:51:10   Config file: config/recommender/notebook_cfg.yaml\n",
      "14:51:10   {'checkpoint': None,\n",
      " 'dataset': {'class': 'LastFM',\n",
      "             'root': '/itet-stor/trachsele/net_scratch/tl4rec/model_outputs/data'},\n",
      " 'model': {'backbone_model': {'embedding_item': {'dropout_rate': 0.1,\n",
      "                                                 'hidden_dims': [32, 32],\n",
      "                                                 'use_dropout': False,\n",
      "                                                 'use_layer_norm': False},\n",
      "                              'embedding_user': {'dropout_rate': 0.1,\n",
      "                                                 'hidden_dims': [32, 32],\n",
      "                                                 'use_dropout': False,\n",
      "                                                 'use_layer_norm': False},\n",
      "                              'simple_model': {'aggregate_func': 'sum',\n",
      "                                               'class': 'SimpleNBFNet',\n",
      "                                               'hidden_dims': [32,\n",
      "                                                               32,\n",
      "                                                               32,\n",
      "                                                               32,\n",
      "                                                               32,\n",
      "                                                               32,\n",
      "                                                               32],\n",
      "                                               'input_dim': 32,\n",
      "                                               'layer_norm': True,\n",
      "                                               'message_func': 'distmult',\n",
      "                                               'short_cut': True}},\n",
      "           'class': 'Gru-Ultra',\n",
      "           'item_projection': {'dropout_rate': 0.1,\n",
      "                               'hidden_dims': [32],\n",
      "                               'use_dropout': True,\n",
      "                               'use_layer_norm': True},\n",
      "           'node_features': False,\n",
      "           'user_projection': {'dropout_rate': 0.1,\n",
      "                               'hidden_dims': [32],\n",
      "                               'use_dropout': True,\n",
      "                               'use_layer_norm': True}},\n",
      " 'optimizer': {'class': 'AdamW', 'lr': 0.00017783663799881164},\n",
      " 'output_dir': '/itet-stor/trachsele/net_scratch/tl4rec/model_outputs/logs',\n",
      " 'task': {'adversarial_temperature': 1,\n",
      "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10', 'ndcg'],\n",
      "          'name': 'TransductiveInference',\n",
      "          'num_negative': 54,\n",
      "          'strict_negative': True},\n",
      " 'train': {'batch_per_epoch': 10,\n",
      "           'batch_size': 16,\n",
      "           'fine_tuning': {'num_epoch_proj': 1},\n",
      "           'gpus': [0],\n",
      "           'gradient_clip': False,\n",
      "           'init_linear_weights': True,\n",
      "           'log_interval': 100,\n",
      "           'loss': 'bce',\n",
      "           'num_epoch': 1,\n",
      "           'num_evals': 10,\n",
      "           'save_ckpt': False,\n",
      "           'save_results_db': True,\n",
      "           'target_metric': 'ndcg',\n",
      "           'test_batch_size': 8,\n",
      "           'wandb': False}}\n",
      "14:51:10   LastFM dataset\n",
      "14:51:10   #train: 39717, #valid: 10671, #test: 12596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will evaluate vs 1000 negatives\n",
      "bpe = 381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:51:10   ------------------------------\n",
      "14:51:10   Number of parameters: 81505\n",
      "14:51:10   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "14:51:10   Epoch 0 begin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discarded node_features\n",
      "discarded edge_features\n",
      "Load rspmm extension. This may take a while...\n"
     ]
    }
   ],
   "source": [
    "%run script/run.py -c config/recommender/notebook_cfg.yaml --dataset LastFM --epochs 1 --bpe 10 --gpus \"[0]\" --ckpt null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667960bc-c845-42a2-b821-68afbfd13555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run script/run.py -c config/recommender/notebook_cfg.yaml --dataset Yelp18 --epochs 10 --bpe 50000 --gpus \"[0]\" --ckpt null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700fa38-a07d-4f5a-b665-c709694fb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run script/run.py -c config/recommender/notebook_cfg.yaml --dataset Yelp18 --epochs 4 --bpe 10 --gpus \"[0]\" --ckpt /itet-stor/trachsele/net_scratch/tl4rec/ckpts/Beauty_Games_2025-01-16_04-20.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51668e-e2a1-4613-be35-3305a3e138d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(np.__version__)  # Check NumPy version\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f89f1e-7b75-491c-9c8d-303b939e0a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ckpt, dataset, epochs, bpe, FT, valid_mr, valid_mrr, valid_hits@1, valid_hits@3, valid_hits@10, valid_ndcg@10, test_mr, test_mrr, test_hits@1, test_hits@3, test_hits@10, test_ndcg@10, valid_ndcg_20, test_ndcg_20]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Define the database file path\n",
    "DB_FILE = \"//itet-stor/trachsele/net_scratch/tl4rec/model_outputs/result.db\"\n",
    "\n",
    "# Connect to the database\n",
    "with sqlite3.connect(DB_FILE) as conn:\n",
    "    # Read the table into a Pandas DataFrame\n",
    "    query = \"SELECT * FROM results\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77512c9d-00a8-40e8-8f75-ec3e7993b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: ALTER TABLE results RENAME COLUMN test_ndcg_10 TO \"test_ndcg@10\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN test_hits_1 TO \"test_hits@1\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN test_hits_3 TO \"test_hits@3\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN test_hits_10 TO \"test_hits@10\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN valid_ndcg_10 TO \"valid_ndcg@10\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN valid_hits_1 TO \"valid_hits@1\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN valid_hits_3 TO \"valid_hits@3\";\n",
      "Executing: ALTER TABLE results RENAME COLUMN valid_hits_10 TO \"valid_hits@10\";\n",
      "Columns renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Path to your SQLite database file\n",
    "DB_FILE = \"//itet-stor/trachsele/net_scratch/tl4rec/model_outputs/result.db\"\n",
    "\n",
    "# Mapping of old column names to new column names with '@'\n",
    "rename_mapping = {\n",
    "    \"test_ndcg_10\": \"test_ndcg@10\",\n",
    "    \"test_hits_1\": \"test_hits@1\",\n",
    "    \"test_hits_3\": \"test_hits@3\",\n",
    "    \"test_hits_10\": \"test_hits@10\",\n",
    "    \"valid_ndcg_10\": \"valid_ndcg@10\",\n",
    "    \"valid_hits_1\": \"valid_hits@1\",\n",
    "    \"valid_hits_3\": \"valid_hits@3\",\n",
    "    \"valid_hits_10\": \"valid_hits@10\"\n",
    "}\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(DB_FILE)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Loop over each column to rename\n",
    "for old_name, new_name in rename_mapping.items():\n",
    "    # Construct the SQL query. The new column name is enclosed in double quotes.\n",
    "    sql = f'ALTER TABLE results RENAME COLUMN {old_name} TO \"{new_name}\";'\n",
    "    print(f\"Executing: {sql}\")\n",
    "    cursor.execute(sql)\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Columns renamed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c99d543-9391-44e0-90cc-917ca2c0ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully at //itet-stor/trachsele/net_scratch/tl4rec/model_outputs/result.db with an empty 'results' table.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the database file path\n",
    "DB_FILE = \"//itet-stor/trachsele/net_scratch/tl4rec/model_outputs/result.db\"\n",
    "\n",
    "# Ensure the directory exists before writing\n",
    "Path(DB_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the schema for the results table\n",
    "CREATE_TABLE_QUERY = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS results (\n",
    "    ckpt TEXT,\n",
    "    dataset TEXT,\n",
    "    epochs INTEGER,\n",
    "    bpe INTEGER,\n",
    "    FT TEXT,\n",
    "    valid_mr REAL,\n",
    "    valid_mrr REAL,\n",
    "    valid_hits_1 REAL,\n",
    "    valid_hits_3 REAL,\n",
    "    valid_hits_10 REAL,\n",
    "    valid_ndcg_10 REAL,\n",
    "    test_mr REAL,\n",
    "    test_mrr REAL,\n",
    "    test_hits_1 REAL,\n",
    "    test_hits_3 REAL,\n",
    "    test_hits_10 REAL,\n",
    "    test_ndcg_10 REAL,\n",
    "    valid_ndcg_20 REAL,\n",
    "    test_ndcg_20 REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Create the database and table\n",
    "with sqlite3.connect(DB_FILE) as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(CREATE_TABLE_QUERY)\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Database created successfully at {DB_FILE} with an empty 'results' table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5c8dba-5c11-46ab-872d-bf66245d1a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated checkpoint saved to: /itet-stor/trachsele/net_scratch/tl4rec/ckpts/pretrain/BookX.pth\n",
      "Updated checkpoint saved to: /itet-stor/trachsele/net_scratch/tl4rec/ckpts/pretrain/Epinions.pth\n",
      "Updated checkpoint saved to: /itet-stor/trachsele/net_scratch/tl4rec/ckpts/pretrain/LastFM.pth\n",
      "Updated checkpoint saved to: /itet-stor/trachsele/net_scratch/tl4rec/ckpts/pretrain/Ml1m.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def remove_prefix(state_dict, prefix=\"ultra.\"):\n",
    "    \"\"\"Remove a prefix from all keys in the state dict.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        # If the key starts with the prefix, remove it.\n",
    "        new_key = key[len(prefix):] if key.startswith(prefix) else key\n",
    "        new_state_dict[new_key] = value\n",
    "    return new_state_dict\n",
    "\n",
    "def correct_checkpoint(ckpt_path, new_ckpt_path=None):\n",
    "    # Load the checkpoint (it may either be the state dict or a dict containing 'state_dict')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    \n",
    "    # Check if the checkpoint is wrapped in a dict under the key 'state_dict'\n",
    "    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n",
    "        original_state_dict = checkpoint[\"state_dict\"]\n",
    "        checkpoint[\"state_dict\"] = remove_prefix(original_state_dict, prefix=\"ultra.\")\n",
    "    else:\n",
    "        # Assume the checkpoint is just a state dict\n",
    "        checkpoint = remove_prefix(checkpoint, prefix=\"ultra.\")\n",
    "    \n",
    "    # Save the corrected checkpoint, either overwriting the original or to a new file.\n",
    "    new_ckpt_path = new_ckpt_path or ckpt_path\n",
    "    torch.save(checkpoint, new_ckpt_path)\n",
    "    print(f\"Updated checkpoint saved to: {new_ckpt_path}\")\n",
    "\n",
    "# Directory containing the checkpoint files\n",
    "ckpt_dir = \"/itet-stor/trachsele/net_scratch/tl4rec/ckpts/pretrain\"\n",
    "ckpt_files = [\"BookX.pth\", \"Epinions.pth\", \"LastFM.pth\", \"Ml1m.pth\"]\n",
    "\n",
    "# Process each checkpoint file.\n",
    "for filename in ckpt_files:\n",
    "    ckpt_path = os.path.join(ckpt_dir, filename)\n",
    "    correct_checkpoint(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb5605-b496-4e33-885b-5e18ede064eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba_bugfix",
   "language": "python",
   "name": "ba_bugfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
