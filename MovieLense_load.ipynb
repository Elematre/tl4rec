{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e420fee2-0240-4834-a5d7-55c66184323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/trachsele/net_scratch/conda_envs/ba_bugfix/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import MovieLens1M, MovieLens100K\n",
    "\n",
    "\n",
    "# Load MovieLens 1M dataset (adjust root if needed)\n",
    "#dataset = MovieLens1M(root='/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/temp_pyg')\n",
    "dataset= MovieLens100K(root=\"/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/temp_pyg\")\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b4ec27-380c-4209-84f9-8e31320a28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60bc3c5-a16b-4780-8342-c6f78b21f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  movie={ x=[1682, 18] },\n",
      "  user={ x=[943, 24] },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 80000],\n",
      "    rating=[80000],\n",
      "    time=[80000],\n",
      "    edge_label_index=[2, 20000],\n",
      "    edge_label=[20000],\n",
      "  },\n",
      "  (movie, rated_by, user)={\n",
      "    edge_index=[2, 80000],\n",
      "    rating=[80000],\n",
      "    time=[80000],\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/trachsele/net_scratch/conda_envs/ba_bugfix/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "graph = dataset[0]\n",
    "dataset[0].num_nodes\n",
    "print (dataset.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc1ee2ba-80b1-40f3-9134-f0281e4a73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  movie={ x=[1682, 18] },\n",
      "  user={ x=[943, 24] },\n",
      "  (user, rates, movie)={\n",
      "    edge_index=[2, 80000],\n",
      "    rating=[80000],\n",
      "    time=[80000],\n",
      "    edge_label_index=[2, 20000],\n",
      "    edge_label=[20000],\n",
      "  },\n",
      "  (movie, rated_by, user)={\n",
      "    edge_index=[2, 80000],\n",
      "    rating=[80000],\n",
      "    time=[80000],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5eadb8-ccb4-4c1d-ad7c-fb58c3a013c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "2624\n",
      "2625\n",
      "All user and movie IDs are within the valid range.\n"
     ]
    }
   ],
   "source": [
    "edge_index = graph['user', 'rates', 'movie'].edge_index\n",
    "# Assuming 'edge_index' contains [user_ids, movie_ids] and num_nodes is the total number of nodes\n",
    "num_nodes = dataset[0].num_nodes\n",
    "user_ids = edge_index[0]\n",
    "user_max_id = user_ids.max().item()  # Maximum user ID\n",
    "movie_ids = edge_index[1] +  user_max_id + 1\n",
    "\n",
    "# Get the maximum number of users (should be user_ids.max()) and total number of nodes\n",
    "\n",
    "movie_min_id = movie_ids.min().item()  # Minimum movie ID\n",
    "movie_max_id = movie_ids.max().item()  # Maximum movie ID\n",
    "print (movie_min_id)\n",
    "print (movie_max_id)\n",
    "print (num_nodes)\n",
    "# Check if the IDs are in the correct range\n",
    "valid_user_ids = (user_ids >= 0).all() and (user_ids <= user_max_id).all()\n",
    "valid_movie_ids = (movie_ids > user_max_id).all() and (movie_ids < num_nodes).all()\n",
    "\n",
    "if valid_user_ids and valid_movie_ids:\n",
    "    print(\"All user and movie IDs are within the valid range.\")\n",
    "else:\n",
    "    if not valid_user_ids:\n",
    "        print(f\"Error: User IDs are out of range. User IDs should be between 0 and {user_max_id}.\")\n",
    "    if not valid_movie_ids:\n",
    "        print(f\"Error: Movie IDs are out of range. Movie IDs should be between {user_max_id + 1} and {num_nodes - 1}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98371c13-d2c7-430d-8715-d0ca90ab48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming graph is your loaded dataset object\n",
    "edge_index = graph['user', 'rates', 'movie'].edge_index\n",
    "\n",
    "# Extract the user IDs and movie IDs from the edge index\n",
    "# edge_index[0] contains user IDs (source nodes)\n",
    "# edge_index[1] contains movie IDs (target nodes)\n",
    "user_ids = edge_index[0].unique()\n",
    "movie_ids = edge_index[1].unique()\n",
    "\n",
    "intersection_mask = torch.isin(user_ids, movie_ids)\n",
    "\n",
    "# Get the actual intersecting IDs\n",
    "intersection = user_ids[intersection_mask]\n",
    "\n",
    "if intersection.numel() > 0:\n",
    "    print(f\"Users and movies share the same IDs: {intersection}\")\n",
    "else:\n",
    "    print(\"No users share the same IDs with movies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7fb8d5-57e5-47e7-a873-1faef5a013de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ratings: 5\n",
      "Unique ratings: tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Get the ratings\n",
    "ratings = dataset[0]['user', 'rates', 'movie'].rating\n",
    "\n",
    "# Find the unique ratings\n",
    "unique_ratings = torch.unique(ratings)\n",
    "\n",
    "# Count the number of unique ratings\n",
    "num_unique_ratings = unique_ratings.size(0)\n",
    "\n",
    "# Print the result\n",
    "print(f'Number of unique ratings: {num_unique_ratings}')\n",
    "print(f'Unique ratings: {unique_ratings}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56fd6e07-ceb2-42fb-83b9-2e8d939d5da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 1176), (0, 0, 655), (0, 0, 902), (0, 0, 3339), (0, 0, 2286), (0, 0, 1179), (0, 0, 1267), (0, 0, 2735), (0, 0, 590), (0, 0, 907)]\n"
     ]
    }
   ],
   "source": [
    "#reading the edges\n",
    "edges = graph['user', 'rates', 'movie'].edge_index\n",
    "\n",
    "triplets = []\n",
    "for i in range(edges.size(1)):\n",
    "    u, v = edges[:, i]  # Get the user and movie node index\n",
    "    triplets.append((u.item(),0, v.item()))\n",
    "\n",
    "print(triplets[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "add007f2-af90-4f14-ab17-71747cd0d79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(triplets)\n",
    "num_train = int(0.8 * len(triplets))\n",
    "num_val = int(0.1 * len(triplets))\n",
    "\n",
    "train_triplets = triplets[:num_train]\n",
    "val_triplets = triplets[num_train:num_train + num_val]\n",
    "test_triplets = triplets[num_train + num_val:]\n",
    "\n",
    "\n",
    "# Save them into separate files\n",
    "def save_triplets(filename, triplets):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for u, v, e in triplets:\n",
    "            f.write(f\"{u}\\t{v}\\t{e}\\n\")\n",
    "\n",
    "save_triplets(\"train.txt\", train_triplets)\n",
    "save_triplets(\"valid.txt\", val_triplets)\n",
    "save_triplets(\"test.txt\", test_triplets)\n",
    "print (\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d51b938-df62-4402-bbd5-f3832f447c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d2588a-3d3a-4878-8a13-a10a92d70c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:42   Random seed: 1024\n",
      "15:47:42   Config file: config/transductive/inference.yaml\n",
      "15:47:42   {'checkpoint': '/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ckpts/ultra_4g.pth',\n",
      " 'dataset': {'class': 'WN18RR', 'root': '~/git/ULTRA/kg-datasets/'},\n",
      " 'model': {'class': 'Ultra',\n",
      "           'entity_model': {'aggregate_func': 'sum',\n",
      "                            'class': 'EntityNBFNet',\n",
      "                            'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
      "                            'input_dim': 64,\n",
      "                            'layer_norm': True,\n",
      "                            'message_func': 'distmult',\n",
      "                            'short_cut': True},\n",
      "           'relation_model': {'aggregate_func': 'sum',\n",
      "                              'class': 'RelNBFNet',\n",
      "                              'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
      "                              'input_dim': 64,\n",
      "                              'layer_norm': True,\n",
      "                              'message_func': 'distmult',\n",
      "                              'short_cut': True}},\n",
      " 'optimizer': {'class': 'AdamW', 'lr': 0.0005},\n",
      " 'output_dir': '~/git/ULTRA/output',\n",
      " 'task': {'adversarial_temperature': 1,\n",
      "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10'],\n",
      "          'name': 'TransductiveInference',\n",
      "          'num_negative': 256,\n",
      "          'strict_negative': True},\n",
      " 'train': {'batch_per_epoch': None,\n",
      "           'batch_size': 8,\n",
      "           'gpus': [0],\n",
      "           'log_interval': 100,\n",
      "           'num_epoch': 0}}\n",
      "/itet-stor/trachsele/net_scratch/conda_envs/ba_bugfix/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index[1]: tensor([10211, 25525,  3891,  ..., 39607, 39608, 39609])\n",
      "edge_type: tensor([ 3,  9, 10,  ..., 14, 14, 14])\n",
      "edge_index[1] size: torch.Size([173670])\n",
      "edge_type size: torch.Size([173670])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ultra/tasks.py:200: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  Ahh = torch.sparse.mm(EhT, Eh).coalesce()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index[1]: tensor([10211, 25525,  3891,  ..., 39607, 39608, 39609])\n",
      "edge_type: tensor([ 3,  9, 10,  ..., 14, 14, 14])\n",
      "edge_index[1] size: torch.Size([173670])\n",
      "edge_type size: torch.Size([173670])\n",
      "edge_index[1]: tensor([10211, 25525,  3891,  ..., 39607, 39608, 39609])\n",
      "edge_type: tensor([ 3,  9, 10,  ..., 14, 14, 14])\n",
      "edge_index[1] size: torch.Size([173670])\n",
      "edge_type size: torch.Size([173670])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "abort.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/script/run.py:243\u001b[0m\n\u001b[1;32m    240\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(pprint\u001b[38;5;241m.\u001b[39mpformat(cfg))\n\u001b[1;32m    242\u001b[0m task_name \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mtask[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 243\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m device \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_device(cfg)\n\u001b[1;32m    246\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m], dataset[\u001b[38;5;241m1\u001b[39m], dataset[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ultra/util.py:149\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m data_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m ds_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(datasets, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_rank() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    152\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ultra/datasets.py:321\u001b[0m, in \u001b[0;36mWN18RR\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m    319\u001b[0m dataset.num_relations = num_relations * 2\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m--> 321\u001b[0m  raise ValueError(\"abort.\")\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m return dataset\n",
      "\u001b[0;31mValueError\u001b[0m: abort."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec\") \n",
    "%run script/run.py -c config/transductive/inference.yaml --dataset WN18RR --epochs 0 --bpe null --gpus [0] --ckpt /usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ckpts/ultra_4g.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700fa38-a07d-4f5a-b665-c709694fb6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/itet-stor/trachsele/net_scratch/conda_envs/ba_bugfix/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "15:50:28   Random seed: 1024\n",
      "15:50:28   Config file: config/transductive/inference.yaml\n",
      "15:50:28   {'checkpoint': '/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ckpts/ultra_4g.pth',\n",
      " 'dataset': {'class': 'MovieLens100k', 'root': '~/git/ULTRA/kg-datasets/'},\n",
      " 'model': {'class': 'Ultra',\n",
      "           'entity_model': {'aggregate_func': 'sum',\n",
      "                            'class': 'EntityNBFNet',\n",
      "                            'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
      "                            'input_dim': 64,\n",
      "                            'layer_norm': True,\n",
      "                            'message_func': 'distmult',\n",
      "                            'short_cut': True},\n",
      "           'relation_model': {'aggregate_func': 'sum',\n",
      "                              'class': 'RelNBFNet',\n",
      "                              'hidden_dims': [64, 64, 64, 64, 64, 64],\n",
      "                              'input_dim': 64,\n",
      "                              'layer_norm': True,\n",
      "                              'message_func': 'distmult',\n",
      "                              'short_cut': True}},\n",
      " 'optimizer': {'class': 'AdamW', 'lr': 0.0005},\n",
      " 'output_dir': '~/git/ULTRA/output',\n",
      " 'task': {'adversarial_temperature': 1,\n",
      "          'metric': ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10'],\n",
      "          'name': 'TransductiveInference',\n",
      "          'num_negative': 256,\n",
      "          'strict_negative': True},\n",
      " 'train': {'batch_per_epoch': None,\n",
      "           'batch_size': 8,\n",
      "           'gpus': [0],\n",
      "           'log_interval': 100,\n",
      "           'num_epoch': 0}}\n",
      "/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ultra/tasks.py:194: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  Ahh = torch.sparse.mm(EhT, Eh).coalesce()\n",
      "15:50:30   MovieLens100k dataset\n",
      "15:50:30   #train: 64000, #valid: 8000, #test: 8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel graph is built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:50:30   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "15:50:30   Evaluate on valid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load rspmm extension. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:51:23   mr: 204.833\n",
      "15:51:23   mrr: 0.0520594\n",
      "15:51:23   hits@1: 0.0203125\n",
      "15:51:23   hits@3: 0.0455\n",
      "15:51:23   hits@10: 0.107125\n",
      "15:51:23   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "15:51:23   Evaluate on test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%run script/run.py -c config/transductive/inference.yaml --dataset MovieLens100k --epochs 0 --bpe null --gpus [0] --ckpt /usr/itetnas04/data-scratch-01/trachsele/data/tl4rec/ckpts/ultra_4g.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a51668e-e2a1-4613-be35-3305a3e138d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.3\n",
      "2.1.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(np.__version__)  # Check NumPy version\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77512c9d-00a8-40e8-8f75-ec3e7993b842",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(\"/usr/itetnas04/data-scratch-01/trachsele/data/tl4rec\") \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f89f1e-7b75-491c-9c8d-303b939e0a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valid.txt', 'requirements.txt', 'MovieLense_load.ipynb', 'temp_pyg', 'config', '.git', '.gitignore', 'ultra', 'LICENSE', 'script', 'test.txt', 'train.txt', 'ckpts', 'asset', 'README.md', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0230f231-7cd1-493e-a69d-055d78affeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All nodes in valid are in train.\n",
      "All nodes in test are in train.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URLs of the triplet files\n",
    "urls = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/Elematre/tl4rec/refs/heads/main/train.txt\",\n",
    "    \"valid\": \"https://raw.githubusercontent.com/Elematre/tl4rec/refs/heads/main/valid.txt\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/Elematre/tl4rec/refs/heads/main/test.txt\",\n",
    "}\n",
    "\n",
    "# Function to load triplet data from a URL\n",
    "def load_triplets(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we got the data correctly\n",
    "    triplets = [line.strip().split() for line in response.text.splitlines()]\n",
    "    return triplets\n",
    "\n",
    "# Function to extract the set of unique nodes from triplets\n",
    "def get_unique_nodes(triplets):\n",
    "    nodes = set()\n",
    "    for triplet in triplets:\n",
    "        u, e, v = triplet\n",
    "        nodes.add(u)\n",
    "        nodes.add(v)\n",
    "    return nodes\n",
    "\n",
    "# Load triplet data from URLs\n",
    "train_triplets = load_triplets(urls[\"train\"])\n",
    "valid_triplets = load_triplets(urls[\"valid\"])\n",
    "test_triplets = load_triplets(urls[\"test\"])\n",
    "\n",
    "# Extract unique nodes\n",
    "train_nodes = get_unique_nodes(train_triplets)\n",
    "valid_nodes = get_unique_nodes(valid_triplets)\n",
    "test_nodes = get_unique_nodes(test_triplets)\n",
    "\n",
    "# Find missing nodes in valid and test datasets\n",
    "missing_in_valid = valid_nodes - train_nodes\n",
    "missing_in_test = test_nodes - train_nodes\n",
    "\n",
    "# Print results\n",
    "if missing_in_valid:\n",
    "    print(f\"Nodes in valid but not in train: {missing_in_valid}\")\n",
    "else:\n",
    "    print(\"All nodes in valid are in train.\")\n",
    "\n",
    "if missing_in_test:\n",
    "    print(f\"Nodes in test but not in train: {missing_in_test}\")\n",
    "else:\n",
    "    print(\"All nodes in test are in train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99d543-9391-44e0-90cc-917ca2c0ee8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ba_bugfix)",
   "language": "python",
   "name": "ba_bugfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
