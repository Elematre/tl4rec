output_dir: /itet-stor/trachsele/net_scratch/tl4rec/model_outputs/logs

dataset:
  class: {{ dataset }}
  root: /itet-stor/trachsele/net_scratch/tl4rec/model_outputs/data

#the input dim of the entity_model needs to be output_dim relation_model + output_dim embedding_model
# projection_dim = embedding_dim
# simple_model_dim % edge_dim == 0!
 
model:
  class: Gru-Ultra
  node_features: no
  edge_features: yes
  user_projection:
   use_dropout: yes
   dropout_rate: 0.1
   use_layer_norm: no
   hidden_dims: [32]
  item_projection: 
   use_dropout: yes
   dropout_rate: 0.1
   use_layer_norm: yes
   hidden_dims: [32]
  edge_projection: 
   use_dropout: yes
   dropout_rate: 0.3743907806104132
   use_layer_norm: no
   hidden_dims: [16,16]
  backbone_model:
    simple_model:
      class: SimpleNBFNet
      input_dim: 32
      hidden_dims: [32, 32, 32, 32, 32, 32, 32]
      message_func: distmult
      aggregate_func: sum
      short_cut: yes
      layer_norm: yes
      project_conv_emb: yes
      # CONSTRAINT: entity_model_inputdim = node_embedding_output_dim +  relation_embedding_output_dim
    embedding_user:
      use_dropout: no
      dropout_rate: 0.1
      use_layer_norm: no
      hidden_dims: [16, 16]
    embedding_item:
      use_dropout: no
      dropout_rate: 0.1
      use_layer_norm: no
      hidden_dims: [16, 16]
    embedding_edge:
      use_dropout: yes
      dropout_rate: 0.03247182903513618
      use_layer_norm: no
      hidden_dims: [16, 16, 16, 16, 16, 16]
    #0.05


task:
  name: TransductiveInference
  num_negative: 54
  strict_negative: yes
  adversarial_temperature: 1
  metric: [mr, mrr, hits@1, hits@3, hits@10, ndcg@20]
  # note if one changes the k from ndcg this should also be adjusted in the code

optimizer:
  class: AdamW
  projection_user_lr: 1.0e-4
  projection_item_lr: 1.0e-4
  projection_edge_lr: 0.000248786603430893
  backbone_conv_lr: 0.00017783663799881164
  backbone_mlp_user_lr: 5.0e-4
  backbone_mlp_item_lr: 5.0e-4
  backbone_mlp_edge_lr: 0.0003360904636420548

train:
  gpus: {{ gpus }}
  batch_size: 16
  num_epoch: {{ epochs }}
  log_interval: 100
  batch_per_epoch: {{ bpe }}
  loss: bce # Options: Bpr, bce
  target_metric: ndcg@20
  wandb: yes
  gradient_clip: no
  init_linear_weights: yes
  num_evals: 10
  # true num_evals is calculated by: ceil(num_epoch / num_evals)
  test_batch_size: 8
  fine_tuning:
    num_epoch_proj: 0

checkpoint: {{ ckpt }}
